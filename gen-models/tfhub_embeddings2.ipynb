{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from comet_ml import Experiment\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/tweets_50_shuffled_train.csv')\n",
    "test_df = pd.read_csv('../data/tweets_50_shuffled_test.csv')\n",
    "X_train = train_df['text_tokenized']\n",
    "y_train = train_df['label']\n",
    "X_test = test_df['text_tokenized']\n",
    "y_test = test_df['label']\n",
    "\n",
    "class_names = y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/henrystoll/nlp-split/37267a1079794d92ac6e338015782dee\n",
      "\n",
      "COMET INFO: Ignoring automatic log_parameter('verbose') because 'keras:verbose' is in COMET_LOGGING_PARAMETERS_IGNORE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 50)                48190600  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 48,197,515\n",
      "Trainable params: 6,915\n",
      "Non-trainable params: 48,190,600\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "59/59 [==============================] - 7s 87ms/step - loss: 1.2373 - acc: 0.5094 - val_loss: 1.4753 - val_acc: 0.3311\n",
      "Epoch 2/200\n",
      "59/59 [==============================] - 4s 72ms/step - loss: 1.0918 - acc: 0.6094 - val_loss: 1.2282 - val_acc: 0.5822\n",
      "Epoch 3/200\n",
      "59/59 [==============================] - 4s 71ms/step - loss: 1.0214 - acc: 0.6471 - val_loss: 1.1873 - val_acc: 0.5975\n",
      "Epoch 4/200\n",
      "59/59 [==============================] - 4s 73ms/step - loss: 0.9625 - acc: 0.6821 - val_loss: 1.3375 - val_acc: 0.4146\n",
      "Epoch 5/200\n",
      "59/59 [==============================] - 4s 68ms/step - loss: 0.9148 - acc: 0.7081 - val_loss: 1.6851 - val_acc: 0.3149\n",
      "Epoch 6/200\n",
      "59/59 [==============================] - 4s 70ms/step - loss: 0.8869 - acc: 0.7182 - val_loss: 1.6239 - val_acc: 0.3567\n",
      "Epoch 7/200\n",
      "59/59 [==============================] - 4s 70ms/step - loss: 0.8590 - acc: 0.7325 - val_loss: 1.6335 - val_acc: 0.3639\n",
      "Epoch 8/200\n",
      "59/59 [==============================] - 4s 67ms/step - loss: 0.8314 - acc: 0.7442 - val_loss: 1.2536 - val_acc: 0.6024\n",
      "Epoch 9/200\n",
      "59/59 [==============================] - 4s 68ms/step - loss: 0.8122 - acc: 0.7529 - val_loss: 1.3213 - val_acc: 0.4389\n",
      "Epoch 10/200\n",
      "59/59 [==============================] - 4s 67ms/step - loss: 0.7847 - acc: 0.7650 - val_loss: 1.1057 - val_acc: 0.6101\n",
      "Epoch 11/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.7581 - acc: 0.7772 - val_loss: 1.0557 - val_acc: 0.6415\n",
      "Epoch 12/200\n",
      "59/59 [==============================] - 4s 75ms/step - loss: 0.7434 - acc: 0.7809 - val_loss: 1.0829 - val_acc: 0.6119\n",
      "Epoch 13/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.7220 - acc: 0.7926 - val_loss: 1.0504 - val_acc: 0.6213\n",
      "Epoch 14/200\n",
      "59/59 [==============================] - 4s 69ms/step - loss: 0.7072 - acc: 0.7970 - val_loss: 1.1185 - val_acc: 0.6069\n",
      "Epoch 15/200\n",
      "59/59 [==============================] - 4s 67ms/step - loss: 0.6932 - acc: 0.7996 - val_loss: 1.1136 - val_acc: 0.6056\n",
      "Epoch 16/200\n",
      "59/59 [==============================] - 4s 71ms/step - loss: 0.6785 - acc: 0.8060 - val_loss: 1.2305 - val_acc: 0.5355\n",
      "Epoch 17/200\n",
      "59/59 [==============================] - 4s 64ms/step - loss: 0.6649 - acc: 0.8143 - val_loss: 1.0987 - val_acc: 0.6105\n",
      "Epoch 18/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.6496 - acc: 0.8159 - val_loss: 1.0914 - val_acc: 0.6204\n",
      "Epoch 19/200\n",
      "59/59 [==============================] - 4s 68ms/step - loss: 0.6379 - acc: 0.8221 - val_loss: 1.2232 - val_acc: 0.5809\n",
      "Epoch 20/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.6281 - acc: 0.8259 - val_loss: 1.1731 - val_acc: 0.5988\n",
      "Epoch 21/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.6143 - acc: 0.8320 - val_loss: 1.2335 - val_acc: 0.5660\n",
      "Epoch 22/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.6068 - acc: 0.8305 - val_loss: 1.2003 - val_acc: 0.6159\n",
      "Epoch 23/200\n",
      "59/59 [==============================] - 4s 65ms/step - loss: 0.5964 - acc: 0.8371 - val_loss: 1.1344 - val_acc: 0.5961\n",
      "Epoch 24/200\n",
      "59/59 [==============================] - 4s 67ms/step - loss: 0.5835 - acc: 0.8398 - val_loss: 1.2113 - val_acc: 0.5813\n",
      "Epoch 25/200\n",
      "59/59 [==============================] - 4s 67ms/step - loss: 0.5720 - acc: 0.8466 - val_loss: 1.2472 - val_acc: 0.5800\n",
      "Epoch 26/200\n",
      "59/59 [==============================] - 4s 65ms/step - loss: 0.5730 - acc: 0.8458 - val_loss: 1.2978 - val_acc: 0.5809\n",
      "Epoch 27/200\n",
      "59/59 [==============================] - 4s 67ms/step - loss: 0.5547 - acc: 0.8531 - val_loss: 1.1913 - val_acc: 0.6006\n",
      "Epoch 28/200\n",
      "59/59 [==============================] - 4s 65ms/step - loss: 0.5448 - acc: 0.8540 - val_loss: 2.0295 - val_acc: 0.2740\n",
      "Epoch 29/200\n",
      "59/59 [==============================] - 4s 70ms/step - loss: 0.5523 - acc: 0.8523 - val_loss: 1.1527 - val_acc: 0.6110\n",
      "Epoch 30/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.5306 - acc: 0.8609 - val_loss: 1.2516 - val_acc: 0.5881\n",
      "Epoch 31/200\n",
      "59/59 [==============================] - 4s 65ms/step - loss: 0.5210 - acc: 0.8654 - val_loss: 1.2891 - val_acc: 0.6033\n",
      "Epoch 32/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.5188 - acc: 0.8672 - val_loss: 1.1985 - val_acc: 0.5939\n",
      "Epoch 33/200\n",
      "59/59 [==============================] - 4s 64ms/step - loss: 0.5046 - acc: 0.8680 - val_loss: 1.6598 - val_acc: 0.5216\n",
      "Epoch 34/200\n",
      "59/59 [==============================] - 4s 68ms/step - loss: 0.5066 - acc: 0.8679 - val_loss: 1.3240 - val_acc: 0.5845\n",
      "Epoch 35/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.4972 - acc: 0.8757 - val_loss: 1.1824 - val_acc: 0.6092\n",
      "Epoch 36/200\n",
      "59/59 [==============================] - 4s 65ms/step - loss: 0.4852 - acc: 0.8765 - val_loss: 1.3306 - val_acc: 0.5916\n",
      "Epoch 37/200\n",
      "59/59 [==============================] - 4s 68ms/step - loss: 0.4843 - acc: 0.8768 - val_loss: 1.2208 - val_acc: 0.5898\n",
      "Epoch 38/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.4718 - acc: 0.8826 - val_loss: 1.1730 - val_acc: 0.6015\n",
      "Epoch 39/200\n",
      "59/59 [==============================] - 4s 64ms/step - loss: 0.4698 - acc: 0.8847 - val_loss: 1.7994 - val_acc: 0.4609\n",
      "Epoch 40/200\n",
      "59/59 [==============================] - 4s 65ms/step - loss: 0.4771 - acc: 0.8773 - val_loss: 1.2131 - val_acc: 0.5948\n",
      "Epoch 41/200\n",
      "59/59 [==============================] - 4s 66ms/step - loss: 0.4535 - acc: 0.8895 - val_loss: 1.2179 - val_acc: 0.5984\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00041: early stopping\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 1.0557 - acc: 0.6415\n",
      "acc : 0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/henrystoll/nlp-split/37267a1079794d92ac6e338015782dee\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     test_accuracy             : 0.6415094137191772\n",
      "COMET INFO:     test_loss                 : 1.0557458400726318\n",
      "COMET INFO:     train_acc [41]            : (0.509357750415802, 0.8894574046134949)\n",
      "COMET INFO:     train_batch_acc [246]     : (0.3984375, 0.8991477489471436)\n",
      "COMET INFO:     train_batch_loss [246]    : (0.44588977098464966, 1.402728796005249)\n",
      "COMET INFO:     train_epoch_duration [41] : (3.764460638980381, 7.210416896035895)\n",
      "COMET INFO:     train_loss [41]           : (0.4534895420074463, 1.2372779846191406)\n",
      "COMET INFO:     train_val_acc [41]        : (0.27403414249420166, 0.6415094137191772)\n",
      "COMET INFO:     train_val_loss [41]       : (1.0504297018051147, 2.029522180557251)\n",
      "COMET INFO:     validate_batch_acc [82]   : (0.2798295319080353, 0.65625)\n",
      "COMET INFO:     validate_batch_loss [82]  : (1.0373440980911255, 2.1819257736206055)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     train_trainable_params : 48197515\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Nadam_beta_1        : 0.9\n",
      "COMET INFO:     Nadam_beta_2        : 0.999\n",
      "COMET INFO:     Nadam_decay         : 0.004\n",
      "COMET INFO:     Nadam_epsilon       : 1e-07\n",
      "COMET INFO:     Nadam_learning_rate : 0.001\n",
      "COMET INFO:     Optimizer           : Nadam\n",
      "COMET INFO:     activation          : relu\n",
      "COMET INFO:     batch_size          : 128\n",
      "COMET INFO:     bert_model_name     : https://tfhub.dev/google/nnlm-en-dim50/2\n",
      "COMET INFO:     dropout             : 0.5\n",
      "COMET INFO:     epochs              : 200\n",
      "COMET INFO:     filters             : 64\n",
      "COMET INFO:     kernel_initializer  : he_normal\n",
      "COMET INFO:     kernel_sizes        : [1, 2, 3]\n",
      "COMET INFO:     l2_lambda           : 0.001\n",
      "COMET INFO:     n_convs_parallel    : 3\n",
      "COMET INFO:     optimizer           : nadam\n",
      "COMET INFO:     padding             : same\n",
      "COMET INFO:     pool_size           : 2\n",
      "COMET INFO:     strides             : 1\n",
      "COMET INFO:     train_Nadam_name    : Nadam\n",
      "COMET INFO:     train_steps         : 59\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     confusion-matrix    : 1\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     histogram3d         : 42\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     model graph         : 1\n",
      "COMET INFO:     notebook            : 1\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO:     source_code         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/henrystoll/nlp-split/4afec5223e3048ff8ac19716e9321202\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 128)               124642688 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 124,659,587\n",
      "Trainable params: 16,899\n",
      "Non-trainable params: 124,642,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "59/59 [==============================] - 6s 91ms/step - loss: 1.2059 - acc: 0.5251 - val_loss: 1.4261 - val_acc: 0.3603\n",
      "Epoch 2/200\n",
      "59/59 [==============================] - 5s 86ms/step - loss: 1.0372 - acc: 0.6340 - val_loss: 1.9168 - val_acc: 0.1712\n",
      "Epoch 3/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.9525 - acc: 0.6887 - val_loss: 1.4161 - val_acc: 0.4403\n",
      "Epoch 4/200\n",
      "59/59 [==============================] - 5s 84ms/step - loss: 0.8860 - acc: 0.7230 - val_loss: 1.4142 - val_acc: 0.4704\n",
      "Epoch 5/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.8325 - acc: 0.7498 - val_loss: 1.4769 - val_acc: 0.4551\n",
      "Epoch 6/200\n",
      "59/59 [==============================] - 5s 87ms/step - loss: 0.7924 - acc: 0.7594 - val_loss: 1.4272 - val_acc: 0.4582\n",
      "Epoch 7/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.7539 - acc: 0.7754 - val_loss: 1.6142 - val_acc: 0.4663\n",
      "Epoch 8/200\n",
      "59/59 [==============================] - 5s 86ms/step - loss: 0.7225 - acc: 0.7924 - val_loss: 1.5363 - val_acc: 0.4429\n",
      "Epoch 9/200\n",
      "59/59 [==============================] - 5s 86ms/step - loss: 0.6940 - acc: 0.8044 - val_loss: 1.7163 - val_acc: 0.3863\n",
      "Epoch 10/200\n",
      "59/59 [==============================] - 5s 84ms/step - loss: 0.6684 - acc: 0.8138 - val_loss: 1.5688 - val_acc: 0.4730\n",
      "Epoch 11/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.6531 - acc: 0.8258 - val_loss: 1.8903 - val_acc: 0.4376\n",
      "Epoch 12/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.6483 - acc: 0.8251 - val_loss: 1.6448 - val_acc: 0.4007\n",
      "Epoch 13/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.6036 - acc: 0.8423 - val_loss: 1.7704 - val_acc: 0.4443\n",
      "Epoch 14/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.6034 - acc: 0.8384 - val_loss: 2.3015 - val_acc: 0.3468\n",
      "Epoch 15/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.5854 - acc: 0.8504 - val_loss: 2.2868 - val_acc: 0.2875\n",
      "Epoch 16/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.5691 - acc: 0.8542 - val_loss: 1.7819 - val_acc: 0.4501\n",
      "Epoch 17/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.5425 - acc: 0.8683 - val_loss: 2.4385 - val_acc: 0.3527\n",
      "Epoch 18/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.5618 - acc: 0.8580 - val_loss: 1.6498 - val_acc: 0.4048\n",
      "Epoch 19/200\n",
      "59/59 [==============================] - 5s 84ms/step - loss: 0.5186 - acc: 0.8767 - val_loss: 2.4694 - val_acc: 0.3356\n",
      "Epoch 20/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.5124 - acc: 0.8812 - val_loss: 1.5095 - val_acc: 0.4614\n",
      "Epoch 21/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.4949 - acc: 0.8845 - val_loss: 1.5851 - val_acc: 0.4142\n",
      "Epoch 22/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.4879 - acc: 0.8870 - val_loss: 1.4388 - val_acc: 0.4573\n",
      "Epoch 23/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.4754 - acc: 0.8936 - val_loss: 1.5321 - val_acc: 0.4587\n",
      "Epoch 24/200\n",
      "59/59 [==============================] - 5s 84ms/step - loss: 0.4707 - acc: 0.8909 - val_loss: 1.6608 - val_acc: 0.4160\n",
      "Epoch 25/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.4622 - acc: 0.8947 - val_loss: 1.3751 - val_acc: 0.4641\n",
      "Epoch 26/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.4493 - acc: 0.9020 - val_loss: 1.3188 - val_acc: 0.4753\n",
      "Epoch 27/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.4435 - acc: 0.9040 - val_loss: 1.5175 - val_acc: 0.4061\n",
      "Epoch 28/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.4383 - acc: 0.9002 - val_loss: 3.7031 - val_acc: 0.2031\n",
      "Epoch 29/200\n",
      "59/59 [==============================] - 5s 78ms/step - loss: 0.4767 - acc: 0.8932 - val_loss: 1.2508 - val_acc: 0.4668\n",
      "Epoch 30/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.4217 - acc: 0.9103 - val_loss: 1.6621 - val_acc: 0.5863\n",
      "Epoch 31/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.4393 - acc: 0.9016 - val_loss: 1.2868 - val_acc: 0.4524\n",
      "Epoch 32/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.4032 - acc: 0.9157 - val_loss: 2.3211 - val_acc: 0.3944\n",
      "Epoch 33/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.4153 - acc: 0.9063 - val_loss: 1.4676 - val_acc: 0.6164\n",
      "Epoch 34/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.3995 - acc: 0.9156 - val_loss: 2.0727 - val_acc: 0.4317\n",
      "Epoch 35/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.4077 - acc: 0.9163 - val_loss: 1.2453 - val_acc: 0.4677\n",
      "Epoch 36/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.3792 - acc: 0.9222 - val_loss: 1.2654 - val_acc: 0.4456\n",
      "Epoch 37/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.3741 - acc: 0.9246 - val_loss: 1.2590 - val_acc: 0.5764\n",
      "Epoch 38/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.3704 - acc: 0.9251 - val_loss: 1.3125 - val_acc: 0.5364\n",
      "Epoch 39/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.3694 - acc: 0.9268 - val_loss: 1.2378 - val_acc: 0.5795\n",
      "Epoch 40/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.3559 - acc: 0.9343 - val_loss: 1.2487 - val_acc: 0.5746\n",
      "Epoch 41/200\n",
      "59/59 [==============================] - 5s 86ms/step - loss: 0.3512 - acc: 0.9348 - val_loss: 1.3250 - val_acc: 0.6087\n",
      "Epoch 42/200\n",
      "59/59 [==============================] - 5s 84ms/step - loss: 0.3559 - acc: 0.9293 - val_loss: 1.5618 - val_acc: 0.5665\n",
      "Epoch 43/200\n",
      "59/59 [==============================] - 5s 84ms/step - loss: 0.3552 - acc: 0.9304 - val_loss: 1.9125 - val_acc: 0.4111\n",
      "Epoch 44/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.3488 - acc: 0.9336 - val_loss: 1.4135 - val_acc: 0.5238\n",
      "Epoch 45/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.3425 - acc: 0.9347 - val_loss: 1.6476 - val_acc: 0.6186\n",
      "Epoch 46/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.3513 - acc: 0.9288 - val_loss: 1.1870 - val_acc: 0.5845\n",
      "Epoch 47/200\n",
      "59/59 [==============================] - 5s 88ms/step - loss: 0.3273 - acc: 0.9398 - val_loss: 1.3073 - val_acc: 0.5696\n",
      "Epoch 48/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.3239 - acc: 0.9402 - val_loss: 1.7418 - val_acc: 0.4721\n",
      "Epoch 49/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.3275 - acc: 0.9383 - val_loss: 1.3189 - val_acc: 0.5283\n",
      "Epoch 50/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.3271 - acc: 0.9378 - val_loss: 1.3173 - val_acc: 0.5773\n",
      "Epoch 51/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.3104 - acc: 0.9447 - val_loss: 1.1740 - val_acc: 0.5620\n",
      "Epoch 52/200\n",
      "59/59 [==============================] - 5s 79ms/step - loss: 0.3091 - acc: 0.9443 - val_loss: 1.7736 - val_acc: 0.5099\n",
      "Epoch 53/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.3126 - acc: 0.9406 - val_loss: 1.2164 - val_acc: 0.5723\n",
      "Epoch 54/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2995 - acc: 0.9490 - val_loss: 1.1831 - val_acc: 0.5809\n",
      "Epoch 55/200\n",
      "59/59 [==============================] - 5s 84ms/step - loss: 0.2945 - acc: 0.9500 - val_loss: 2.4584 - val_acc: 0.4317\n",
      "Epoch 56/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.3127 - acc: 0.9408 - val_loss: 2.5921 - val_acc: 0.3625\n",
      "Epoch 57/200\n",
      "59/59 [==============================] - 5s 78ms/step - loss: 0.3134 - acc: 0.9428 - val_loss: 2.6519 - val_acc: 0.3711\n",
      "Epoch 58/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.3229 - acc: 0.9374 - val_loss: 1.4923 - val_acc: 0.5503\n",
      "Epoch 59/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.2855 - acc: 0.9541 - val_loss: 1.2536 - val_acc: 0.5804\n",
      "Epoch 60/200\n",
      "59/59 [==============================] - 5s 79ms/step - loss: 0.2793 - acc: 0.9566 - val_loss: 1.4070 - val_acc: 0.6280\n",
      "Epoch 61/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.2899 - acc: 0.9482 - val_loss: 2.4889 - val_acc: 0.5804\n",
      "Epoch 62/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.3144 - acc: 0.9437 - val_loss: 1.2256 - val_acc: 0.5651\n",
      "Epoch 63/200\n",
      "59/59 [==============================] - 5s 84ms/step - loss: 0.2712 - acc: 0.9580 - val_loss: 1.2277 - val_acc: 0.5732\n",
      "Epoch 64/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.2681 - acc: 0.9593 - val_loss: 1.1712 - val_acc: 0.6038\n",
      "Epoch 65/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.2662 - acc: 0.9572 - val_loss: 1.2131 - val_acc: 0.6011\n",
      "Epoch 66/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.2639 - acc: 0.9588 - val_loss: 1.8013 - val_acc: 0.5660\n",
      "Epoch 67/200\n",
      "59/59 [==============================] - 5s 79ms/step - loss: 0.2686 - acc: 0.9562 - val_loss: 1.5559 - val_acc: 0.5804\n",
      "Epoch 68/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2603 - acc: 0.9573 - val_loss: 1.8794 - val_acc: 0.5629\n",
      "Epoch 69/200\n",
      "59/59 [==============================] - 5s 84ms/step - loss: 0.2673 - acc: 0.9565 - val_loss: 1.3144 - val_acc: 0.6379\n",
      "Epoch 70/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2680 - acc: 0.9552 - val_loss: 2.4519 - val_acc: 0.4847\n",
      "Epoch 71/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.2793 - acc: 0.9513 - val_loss: 1.4444 - val_acc: 0.5822\n",
      "Epoch 72/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.2626 - acc: 0.9581 - val_loss: 2.4532 - val_acc: 0.4452\n",
      "Epoch 73/200\n",
      "59/59 [==============================] - 5s 88ms/step - loss: 0.2804 - acc: 0.9507 - val_loss: 1.2197 - val_acc: 0.5773\n",
      "Epoch 74/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2461 - acc: 0.9636 - val_loss: 1.2951 - val_acc: 0.5638\n",
      "Epoch 75/200\n",
      "59/59 [==============================] - 5s 79ms/step - loss: 0.2426 - acc: 0.9649 - val_loss: 1.6224 - val_acc: 0.5795\n",
      "Epoch 76/200\n",
      "59/59 [==============================] - 5s 88ms/step - loss: 0.2419 - acc: 0.9659 - val_loss: 1.6956 - val_acc: 0.5593\n",
      "Epoch 77/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2464 - acc: 0.9607 - val_loss: 1.4379 - val_acc: 0.5845\n",
      "Epoch 78/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.2355 - acc: 0.9671 - val_loss: 4.2009 - val_acc: 0.3702\n",
      "Epoch 79/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.2856 - acc: 0.9544 - val_loss: 1.6145 - val_acc: 0.5593\n",
      "Epoch 80/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.2389 - acc: 0.9646 - val_loss: 1.3084 - val_acc: 0.5620\n",
      "Epoch 81/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.2322 - acc: 0.9677 - val_loss: 1.2244 - val_acc: 0.5867\n",
      "Epoch 82/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.2289 - acc: 0.9692 - val_loss: 1.6325 - val_acc: 0.5782\n",
      "Epoch 83/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2297 - acc: 0.9682 - val_loss: 1.2348 - val_acc: 0.5746\n",
      "Epoch 84/200\n",
      "59/59 [==============================] - 5s 79ms/step - loss: 0.2225 - acc: 0.9723 - val_loss: 1.2729 - val_acc: 0.5840\n",
      "Epoch 85/200\n",
      "59/59 [==============================] - 5s 80ms/step - loss: 0.2206 - acc: 0.9713 - val_loss: 1.2750 - val_acc: 0.5836\n",
      "Epoch 86/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2209 - acc: 0.9705 - val_loss: 1.2522 - val_acc: 0.5692\n",
      "Epoch 87/200\n",
      "59/59 [==============================] - 5s 79ms/step - loss: 0.2177 - acc: 0.9752 - val_loss: 3.9761 - val_acc: 0.4178\n",
      "Epoch 88/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.2873 - acc: 0.9527 - val_loss: 1.4590 - val_acc: 0.5728\n",
      "Epoch 89/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2258 - acc: 0.9693 - val_loss: 1.4114 - val_acc: 0.5800\n",
      "Epoch 90/200\n",
      "59/59 [==============================] - 5s 78ms/step - loss: 0.2270 - acc: 0.9666 - val_loss: 2.7948 - val_acc: 0.5189\n",
      "Epoch 91/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2440 - acc: 0.9592 - val_loss: 1.6508 - val_acc: 0.5674\n",
      "Epoch 92/200\n",
      "59/59 [==============================] - 5s 79ms/step - loss: 0.2142 - acc: 0.9759 - val_loss: 1.3716 - val_acc: 0.5854\n",
      "Epoch 93/200\n",
      "59/59 [==============================] - 5s 79ms/step - loss: 0.2090 - acc: 0.9764 - val_loss: 1.3863 - val_acc: 0.5710\n",
      "Epoch 94/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.2065 - acc: 0.9774 - val_loss: 1.3826 - val_acc: 0.5813\n",
      "Epoch 95/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2055 - acc: 0.9766 - val_loss: 1.3139 - val_acc: 0.5755\n",
      "Epoch 96/200\n",
      "59/59 [==============================] - 5s 81ms/step - loss: 0.2039 - acc: 0.9772 - val_loss: 1.5546 - val_acc: 0.5314\n",
      "Epoch 97/200\n",
      "59/59 [==============================] - 5s 83ms/step - loss: 0.2066 - acc: 0.9747 - val_loss: 1.6777 - val_acc: 0.5705\n",
      "Epoch 98/200\n",
      "59/59 [==============================] - 5s 85ms/step - loss: 0.2231 - acc: 0.9676 - val_loss: 1.3234 - val_acc: 0.5777\n",
      "Epoch 99/200\n",
      "59/59 [==============================] - 5s 82ms/step - loss: 0.2002 - acc: 0.9778 - val_loss: 1.2780 - val_acc: 0.5827\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00099: early stopping\n",
      "70/70 [==============================] - 1s 20ms/step - loss: 1.3144 - acc: 0.6379\n",
      "acc : 0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/henrystoll/nlp-split/4afec5223e3048ff8ac19716e9321202\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     test_accuracy             : 0.6379155516624451\n",
      "COMET INFO:     test_loss                 : 1.3143929243087769\n",
      "COMET INFO:     train_acc [99]            : (0.5251110792160034, 0.977783739566803)\n",
      "COMET INFO:     train_batch_acc [594]     : (0.3203125, 0.9921875)\n",
      "COMET INFO:     train_batch_loss [594]    : (0.1736261546611786, 2.4800643920898438)\n",
      "COMET INFO:     train_epoch_duration [99] : (4.593676803982817, 7.574536058993544)\n",
      "COMET INFO:     train_loss [99]           : (0.2002364546060562, 1.205949068069458)\n",
      "COMET INFO:     train_val_acc [99]        : (0.171159029006958, 0.6379155516624451)\n",
      "COMET INFO:     train_val_loss [99]       : (1.171150803565979, 4.200887680053711)\n",
      "COMET INFO:     validate_batch_acc [198]  : (0.171875, 0.6640625)\n",
      "COMET INFO:     validate_batch_loss [198] : (1.086380124092102, 4.714748859405518)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     train_trainable_params : 124659587\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Nadam_beta_1        : 0.9\n",
      "COMET INFO:     Nadam_beta_2        : 0.999\n",
      "COMET INFO:     Nadam_decay         : 0.004\n",
      "COMET INFO:     Nadam_epsilon       : 1e-07\n",
      "COMET INFO:     Nadam_learning_rate : 0.001\n",
      "COMET INFO:     Optimizer           : Nadam\n",
      "COMET INFO:     activation          : relu\n",
      "COMET INFO:     batch_size          : 128\n",
      "COMET INFO:     bert_model_name     : https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\n",
      "COMET INFO:     dropout             : 0.5\n",
      "COMET INFO:     epochs              : 200\n",
      "COMET INFO:     filters             : 64\n",
      "COMET INFO:     kernel_initializer  : he_normal\n",
      "COMET INFO:     kernel_sizes        : [1, 2, 3]\n",
      "COMET INFO:     l2_lambda           : 0.001\n",
      "COMET INFO:     n_convs_parallel    : 3\n",
      "COMET INFO:     optimizer           : nadam\n",
      "COMET INFO:     padding             : same\n",
      "COMET INFO:     pool_size           : 2\n",
      "COMET INFO:     strides             : 1\n",
      "COMET INFO:     train_Nadam_name    : Nadam\n",
      "COMET INFO:     train_steps         : 59\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     confusion-matrix    : 1\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     histogram3d         : 100\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     model graph         : 1\n",
      "COMET INFO:     notebook            : 1\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO:     source_code         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "tfhub_embedding_models = [\"https://tfhub.dev/google/nnlm-en-dim50/2\", \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"]\n",
    "activation = 'relu'\n",
    "kernel_initializer = 'he_normal'\n",
    "l2_lambda = 1e-3\n",
    "dropout=0.5\n",
    "filters = 64\n",
    "kernel_sizes = [1, 2, 3]\n",
    "n_convs_parallel = len(kernel_sizes)\n",
    "padding='same'\n",
    "pool_size = 2\n",
    "strides = 1\n",
    "epochs = 200\n",
    "optimizer = 'nadam'\n",
    "\n",
    "for embed_model in tfhub_embedding_models:\n",
    "    def make_model():\n",
    "        regularizers = keras.regularizers.l2(l2=l2_lambda)\n",
    "        model = keras.Sequential([\n",
    "            hub.KerasLayer(embed_model, dtype=tf.string, input_shape=[],),\n",
    "            keras.layers.Dense(128, activation=activation, kernel_initializer=kernel_initializer, kernel_regularizer=regularizers),\n",
    "            keras.layers.Dense(len(class_names), activation=\"softmax\")\n",
    "        ])\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(\n",
    "            loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"acc\"]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    project_name = 'nlp_split'\n",
    "    experiment = Experiment(\n",
    "        project_name=project_name,\n",
    "        auto_param_logging=True,\n",
    "        # auto_histogram_weight_logging=True,\n",
    "        auto_histogram_gradient_logging=True,\n",
    "        auto_histogram_activation_logging=True,\n",
    "        api_key=\"HeH9EtfDC2KUlCOjeQaU1CuOM\",\n",
    "        workspace=\"henrystoll\",\n",
    "    )\n",
    "    params = {\n",
    "        'batch_size': batch_size,\n",
    "        'bert_model_name': embed_model,\n",
    "        'filters': filters,\n",
    "        'kernel_sizes': kernel_sizes,\n",
    "        'pool_size': pool_size,\n",
    "        'padding': padding,\n",
    "        'strides': strides,\n",
    "        'n_convs_parallel': n_convs_parallel,\n",
    "        'activation': activation,\n",
    "        'kernel_initializer': kernel_initializer,\n",
    "        'l2_lambda': l2_lambda,\n",
    "        'dropout': dropout,\n",
    "        'optimizer': optimizer,\n",
    "        'epochs': epochs,\n",
    "    }\n",
    "\n",
    "    experiment.log_parameters(params)\n",
    "\n",
    "    model = make_model()\n",
    "    # keras.utils.plot_model(model, \"model.png\", show_shapes=True)\n",
    "    # experiment.log_asset(\"model.png\")\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=30,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True)\n",
    "\n",
    "    with experiment.train():\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            verbose=1,\n",
    "                            callbacks=[early_stopping])\n",
    "\n",
    "    with experiment.test():\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        print('acc : {:.3f}'.format(accuracy))\n",
    "        metrics = {\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        experiment.log_metrics(metrics)\n",
    "\n",
    "    y_predicted = model.predict(X_test)\n",
    "    y_predicted = y_predicted.argmax(axis=1)\n",
    "    experiment.log_confusion_matrix(y_test.to_numpy(), y_predicted)\n",
    "\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
