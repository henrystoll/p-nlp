{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from comet_ml import Experiment\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (7427,)\n",
      "X_test shape:  (2226,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrystoll/SDK/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:15: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"bb_box\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/tweets_50_shuffled_train.csv')\n",
    "test_df = pd.read_csv('../data/tweets_50_shuffled_test.csv')\n",
    "X_train = train_df['text_tokenized']\n",
    "y_train = train_df['label']\n",
    "X_test = test_df['text_tokenized']\n",
    "y_test = test_df['label']\n",
    "\n",
    "class_names = y_train.unique()\n",
    "# print(\"Class names: \", class_names)\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "# print(\"X_val shape: \", X_val.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "# TODO: save\n",
    "sns.countplot(x=y_train)\n",
    "plt.savefig('../img/class_imbalance.png',bb_box='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-7466605857d0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m vectorizer = make_vectorizer(max_tokens=max_tokens,\n\u001B[0;32m---> 62\u001B[0;31m                              output_sequence_length=output_sequence_length)\n\u001B[0m\u001B[1;32m     63\u001B[0m \u001B[0mvocabulary\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvectorizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_vocabulary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[0membedding_layer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmake_embedding_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocabulary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0membedding_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-9-7466605857d0>\u001B[0m in \u001B[0;36mmake_vectorizer\u001B[0;34m(max_tokens, output_sequence_length)\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mvect\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextVectorization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_tokens\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_tokens\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_sequence_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_sequence_length\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mtext_ds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m     \u001B[0mvect\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madapt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_ds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mvect\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/SDK/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py\u001B[0m in \u001B[0;36madapt\u001B[0;34m(self, data, reset_state)\u001B[0m\n\u001B[1;32m    414\u001B[0m               type(data)))\n\u001B[1;32m    415\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 416\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_index_lookup_layer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madapt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpreprocessed_inputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    417\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    418\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mget_vocabulary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/SDK/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/preprocessing/index_lookup.py\u001B[0m in \u001B[0;36madapt\u001B[0;34m(self, data, reset_state)\u001B[0m\n\u001B[1;32m    364\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mreset_state\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    365\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"IndexLookup does not support streaming adapts.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 366\u001B[0;31m     \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mIndexLookup\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madapt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreset_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    367\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    368\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mget_vocabulary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/SDK/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_preprocessing_layer.py\u001B[0m in \u001B[0;36madapt\u001B[0;34m(self, data, batch_size, steps, reset_state)\u001B[0m\n\u001B[1;32m    329\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_adapt_accumulator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_combiner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrestore\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restore_updates\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m     super(CombinerPreprocessingLayer, self).adapt(\n\u001B[0;32m--> 331\u001B[0;31m         data, batch_size=batch_size, steps=steps, reset_state=reset_state)\n\u001B[0m\u001B[1;32m    332\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    333\u001B[0m   def _add_state_variable(self,\n",
      "\u001B[0;32m~/SDK/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_preprocessing_layer.py\u001B[0m in \u001B[0;36madapt\u001B[0;34m(self, data, batch_size, steps, reset_state)\u001B[0m\n\u001B[1;32m    240\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcatch_stop_iteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 242\u001B[0;31m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_adapt_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    243\u001B[0m           \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m             \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/SDK/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_preprocessing_layer.py\u001B[0m in \u001B[0;36madapt_step\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    151\u001B[0m       \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_adapt_maybe_build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 153\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    154\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_steps_per_execution\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/SDK/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    520\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    521\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 522\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    523\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    524\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/SDK/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_preprocessing_layer.py\u001B[0m in \u001B[0;36mupdate_state\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    307\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_adapt_accumulator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_accumulator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    308\u001B[0m     self._adapt_accumulator = self._combiner.compute(data,\n\u001B[0;32m--> 309\u001B[0;31m                                                      self._adapt_accumulator)\n\u001B[0m\u001B[1;32m    310\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    311\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mmerge_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/SDK/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/preprocessing/index_lookup.py\u001B[0m in \u001B[0;36mcompute\u001B[0;34m(self, values, accumulator)\u001B[0m\n\u001B[1;32m    693\u001B[0m         \u001B[0maccumulator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"next_doc_id\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    694\u001B[0m       \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdocument\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 695\u001B[0;31m         \u001B[0maccumulator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    696\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_idf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    697\u001B[0m           \u001B[0mdoc_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maccumulator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mper_doc_count_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "max_tokens = 20_000\n",
    "output_sequence_length=4000\n",
    "embedding_dim = 300\n",
    "\n",
    "\n",
    "def make_vectorizer(max_tokens: int, output_sequence_length: int) -> TextVectorization:\n",
    "    vect = TextVectorization(max_tokens=max_tokens, output_sequence_length=output_sequence_length)\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(X_train.values).batch(batch_size)\n",
    "    vect.adapt(text_ds)\n",
    "    return vect\n",
    "\n",
    "\n",
    "def get_embedding_matrix(embedding_dim, num_tokens, word_index):\n",
    "    path_to_glove_file = f\"glove.6B.{embedding_dim}d.txt\"\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    with open(path_to_glove_file) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(\"Found {} word vectors.\".format(len(embeddings_index)))\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def make_embedding_layer(voc: List[str], embedding_dim: int) -> layers.Embedding:\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    num_tokens = len(voc) + 2\n",
    "\n",
    "    # just testing if word_index works\n",
    "    test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "    print(\"word_index test:\")\n",
    "    print(test, \" --> \", [word_index[w] for w in test])\n",
    "\n",
    "    embedding_matrix = get_embedding_matrix(embedding_dim, num_tokens, word_index)\n",
    "\n",
    "    return layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=True,\n",
    "        mask_zero=True\n",
    "    )\n",
    "\n",
    "\n",
    "vectorizer = make_vectorizer(max_tokens=max_tokens,\n",
    "                             output_sequence_length=output_sequence_length)\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "embedding_layer = make_embedding_layer(vocabulary, embedding_dim=embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "optimizer = 'nadam'\n",
    "activation = 'relu'\n",
    "kernel_initializer = 'he_normal'\n",
    "l2_lambda = 1e-3\n",
    "dropout=0.5\n",
    "filters = 128\n",
    "n_convs_depth = 1\n",
    "# kernel_size = 3\n",
    "kernel_sizes = [1, 2, 3]\n",
    "n_convs_parallel = len(kernel_sizes)\n",
    "padding='same'\n",
    "pool_size = 3\n",
    "# pool_size = 5\n",
    "strides = 1\n",
    "# strides = 2\n",
    "class_weight = None\n",
    "embedding='mask & trainable'\n",
    "\n",
    "def make_model():\n",
    "    regularizers = keras.regularizers.l2(l2=l2_lambda)\n",
    "    convs = []\n",
    "    string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorizer(string_input)\n",
    "    embed = embedding_layer(x)\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # for _ in range(n_convs_depth):\n",
    "        x = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=activation,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=regularizers,\n",
    "            padding=padding,\n",
    "        )(embed)\n",
    "        x = layers.SpatialDropout1D(dropout)(x)\n",
    "        x = layers.MaxPooling1D(pool_size=pool_size, strides=strides)(x)\n",
    "        convs.append(layers.GlobalMaxPooling1D()(x))\n",
    "\n",
    "    x = layers.Concatenate()(convs)\n",
    "    x = layers.Dense(n_convs_parallel*filters, activation=activation, kernel_regularizer=regularizers)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    output_ = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "    model = keras.Model(string_input, output_)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"acc\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "project_name = 'nlp_split'\n",
    "experiment = Experiment(\n",
    "    project_name=project_name,\n",
    "    auto_param_logging=True,\n",
    "    # auto_histogram_weight_logging=True,\n",
    "    auto_histogram_gradient_logging=True,\n",
    "    auto_histogram_activation_logging=True,\n",
    "    api_key=\"HeH9EtfDC2KUlCOjeQaU1CuOM\",\n",
    "    workspace=\"henrystoll\",\n",
    ")\n",
    "params = {\n",
    "    'batch_size': batch_size,\n",
    "    'max_tokens': max_tokens,\n",
    "    'output_sequence_length': output_sequence_length,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'embedding': embedding,\n",
    "    'filters': filters,\n",
    "    'kernel_sizes': kernel_sizes,\n",
    "    'pool_size': pool_size,\n",
    "    'padding': padding,\n",
    "    'strides': strides,\n",
    "    'n_convs_depth': n_convs_depth,\n",
    "    'n_convs_parallel': n_convs_parallel,\n",
    "    'activation': activation,\n",
    "    'kernel_initializer': kernel_initializer,\n",
    "    'l2_lambda': l2_lambda,\n",
    "    'dropout': dropout,\n",
    "    'class_weight': class_weight,\n",
    "    'optimizer': optimizer,\n",
    "    'epochs': epochs,\n",
    "}\n",
    "\n",
    "experiment.log_parameters(params)\n",
    "\n",
    "model = make_model()\n",
    "# keras.utils.plot_model(model, \"model.png\", show_shapes=True)\n",
    "# experiment.log_asset(\"model.png\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "with experiment.train():\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        class_weight=class_weight,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "with experiment.test():\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('acc : {:.3f}'.format(accuracy))\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    experiment.log_metrics(metrics)\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted = y_predicted.argmax(axis=1)\n",
    "experiment.log_confusion_matrix(y_test.to_numpy(), y_predicted)\n",
    "\n",
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}