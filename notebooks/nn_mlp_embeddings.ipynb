{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from comet_ml import Experiment\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tweets_50.csv')\n",
    "X = df['text_tokenized']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
    "class_names = y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word_index test:\n",
      "['the', 'cat', 'sat', 'on', 'the', 'mat']  -->  [4, 1170, 2063, 17, 4, 8835]\n",
      "Found 400000 word vectors.\n",
      "Converted 18523 words (1477 misses)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "max_tokens = 20_000\n",
    "output_sequence_length= 400\n",
    "embedding_dim = 50\n",
    "\n",
    "\n",
    "\n",
    "def make_vectorizer(max_tokens: int, output_sequence_length: int) -> TextVectorization:\n",
    "    vect = TextVectorization(max_tokens=max_tokens, output_sequence_length=output_sequence_length)\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(X_train.values).batch(batch_size)\n",
    "    vect.adapt(text_ds)\n",
    "    return vect\n",
    "\n",
    "\n",
    "def get_embedding_matrix(embedding_dim, num_tokens, word_index):\n",
    "    path_to_glove_file = os.path.join(\n",
    "        os.path.expanduser(\"~\"), f\"Documents/Datasets/glove.6B.{embedding_dim}d.txt\"\n",
    "    )\n",
    "    embeddings_index = {}\n",
    "    with open(path_to_glove_file) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(\"Found {} word vectors.\".format(len(embeddings_index)))\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def make_embedding_layer(voc: List[str], embedding_dim: int) -> layers.Embedding:\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    num_tokens = len(voc) + 2\n",
    "\n",
    "    # just testing if word_index works\n",
    "    test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "    print(\"word_index test:\")\n",
    "    print(test, \" --> \", [word_index[w] for w in test])\n",
    "\n",
    "    embedding_matrix = get_embedding_matrix(embedding_dim, num_tokens, word_index)\n",
    "\n",
    "    return layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "    )\n",
    "\n",
    "\n",
    "vectorizer = make_vectorizer(max_tokens=max_tokens,\n",
    "                             output_sequence_length=output_sequence_length)\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "embedding_layer = make_embedding_layer(vocabulary, embedding_dim=embedding_dim)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "n_layers = 5\n",
    "activation = 'selu'\n",
    "kernel_initializer = 'lecun_normal'\n",
    "l2_lambda = 1e-3\n",
    "dropout=0.2\n",
    "units=2000\n",
    "\n",
    "# class_weight = {0: 1., 1: 1., 2: 1., 3: 1}\n",
    "class_weight = None\n",
    "epochs = 200\n",
    "optimizer = 'nadam'\n",
    "\n",
    "def make_model():\n",
    "    regularizers = keras.regularizers.l2(l2=l2_lambda)\n",
    "    string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorizer(string_input)\n",
    "    embed = embedding_layer(x)\n",
    "    x = layers.Flatten()(embed)\n",
    "    \n",
    "    # create fully connected layer\n",
    "    for _ in range(n_layers):\n",
    "        x = layers.Dense(\n",
    "            units=units,\n",
    "            activation=activation,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=regularizers\n",
    "            )(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    output_ = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "    model = keras.Model(string_input, output_)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"acc\"]\n",
    "    )\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/henrystoll/nlp-embeddings-mlp/ada5e08b4c724dfba45b77f102ea2988\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     train_acc               : 0.39267829060554504\n",
      "COMET INFO:     train_batch_acc [6]     : (0.0859375, 0.4296875)\n",
      "COMET INFO:     train_batch_loss [6]    : (17.834327697753906, 112.7439193725586)\n",
      "COMET INFO:     train_epoch_duration    : 311.7163088310001\n",
      "COMET INFO:     train_loss              : 61.89064407348633\n",
      "COMET INFO:     train_val_acc           : 0.3314344882965088\n",
      "COMET INFO:     train_val_loss          : 19.35145378112793\n",
      "COMET INFO:     validate_batch_acc [2]  : (0.35085228085517883, 0.421875)\n",
      "COMET INFO:     validate_batch_loss [2] : (17.731243133544922, 19.21815299987793)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     train_trainable_params : 401100104\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Nadam_beta_1           : 0.9\n",
      "COMET INFO:     Nadam_beta_2           : 0.999\n",
      "COMET INFO:     Nadam_decay            : 0.004\n",
      "COMET INFO:     Nadam_epsilon          : 1e-07\n",
      "COMET INFO:     Nadam_learning_rate    : 0.001\n",
      "COMET INFO:     Optimizer              : Nadam\n",
      "COMET INFO:     activation             : selu\n",
      "COMET INFO:     batch_size             : 128\n",
      "COMET INFO:     class_weight           : 1\n",
      "COMET INFO:     dropout                : 1\n",
      "COMET INFO:     embedding_dim          : 50\n",
      "COMET INFO:     epochs                 : 200\n",
      "COMET INFO:     kernel_initializer     : lecun_normal\n",
      "COMET INFO:     l2_lambda              : 0.001\n",
      "COMET INFO:     max_tokens             : 20000\n",
      "COMET INFO:     n_layers               : 1\n",
      "COMET INFO:     optimizer              : nadam\n",
      "COMET INFO:     output_sequence_length : 400\n",
      "COMET INFO:     train_Nadam_name       : Nadam\n",
      "COMET INFO:     train_steps            : 46\n",
      "COMET INFO:     units                  : 20000\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     asset                    : 2\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (241 KB)\n",
      "COMET INFO:     histogram3d              : 12\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/henrystoll/nlp-embeddings-mlp/eb7d4fea4aeb4cdaaa402ce9cffa7dce\n",
      "\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_8 (TextVe (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 400, 50)           1000100   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 400)               8000400   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 1604      \n",
      "=================================================================\n",
      "Total params: 9,002,104\n",
      "Trainable params: 8,002,004\n",
      "Non-trainable params: 1,000,100\n",
      "_________________________________________________________________\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 9s 164ms/step - loss: 6.8327 - acc: 0.3960 - val_loss: 5.0961 - val_acc: 0.3366\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 7s 162ms/step - loss: 1.8185 - acc: 0.5819 - val_loss: 1.7852 - val_acc: 0.4801\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 8s 164ms/step - loss: 1.1230 - acc: 0.7230 - val_loss: 3.2810 - val_acc: 0.3511\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 8s 165ms/step - loss: 0.8391 - acc: 0.8078 - val_loss: 3.1982 - val_acc: 0.4790\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 7s 156ms/step - loss: 0.7064 - acc: 0.8475 - val_loss: 3.0853 - val_acc: 0.3672\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 8s 167ms/step - loss: 0.5529 - acc: 0.8998 - val_loss: 2.7964 - val_acc: 0.4770\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 8s 171ms/step - loss: 0.4472 - acc: 0.9359 - val_loss: 2.4010 - val_acc: 0.4045\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 8s 167ms/step - loss: 0.3849 - acc: 0.9549 - val_loss: 2.3157 - val_acc: 0.5034\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 0.3276 - acc: 0.9750 - val_loss: 1.9584 - val_acc: 0.4832\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 8s 170ms/step - loss: 0.2937 - acc: 0.9831 - val_loss: 2.1124 - val_acc: 0.5065\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 8s 170ms/step - loss: 0.2932 - acc: 0.9779 - val_loss: 2.0940 - val_acc: 0.4842\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 1.1263 - acc: 0.8745 - val_loss: 1.4999 - val_acc: 0.4775\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 0.7875 - acc: 0.7930 - val_loss: 1.7902 - val_acc: 0.4656\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 8s 165ms/step - loss: 0.4495 - acc: 0.9397 - val_loss: 2.0799 - val_acc: 0.5075\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 0.3530 - acc: 0.9753 - val_loss: 1.8979 - val_acc: 0.4956\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 8s 175ms/step - loss: 0.3015 - acc: 0.9883 - val_loss: 1.8783 - val_acc: 0.4780\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 0.2783 - acc: 0.9902 - val_loss: 1.9957 - val_acc: 0.4764\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 7s 163ms/step - loss: 0.9271 - acc: 0.8691 - val_loss: 2.7773 - val_acc: 0.4816\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 7s 157ms/step - loss: 0.6558 - acc: 0.8492 - val_loss: 1.7856 - val_acc: 0.4904\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.3707 - acc: 0.9694 - val_loss: 1.8144 - val_acc: 0.5075\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.3109 - acc: 0.9864 - val_loss: 1.9288 - val_acc: 0.5085\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.2826 - acc: 0.9870 - val_loss: 1.9496 - val_acc: 0.4630\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.2926 - acc: 0.9757 - val_loss: 7.0246 - val_acc: 0.4821\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.7135 - acc: 0.8482 - val_loss: 1.8792 - val_acc: 0.5148\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 7s 157ms/step - loss: 0.3509 - acc: 0.9687 - val_loss: 2.3562 - val_acc: 0.5111\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.3559 - acc: 0.9601 - val_loss: 1.9454 - val_acc: 0.4749\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.2805 - acc: 0.9870 - val_loss: 2.1421 - val_acc: 0.5075\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.2555 - acc: 0.9900 - val_loss: 1.8800 - val_acc: 0.5091\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.2314 - acc: 0.9941 - val_loss: 1.8808 - val_acc: 0.4987\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.2170 - acc: 0.9931 - val_loss: 2.1280 - val_acc: 0.5034\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.5859 - acc: 0.9320 - val_loss: 2.9585 - val_acc: 0.4050\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 7s 155ms/step - loss: 1.0262 - acc: 0.6793 - val_loss: 1.6239 - val_acc: 0.4982\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 7s 159ms/step - loss: 0.5117 - acc: 0.9137 - val_loss: 1.8254 - val_acc: 0.5049\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 0.3597 - acc: 0.9713 - val_loss: 1.7801 - val_acc: 0.5080\n",
      "COMET WARNING: keras layer.weights and layer.get_weights() are different lengths; ignoring weight histogram\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00034: early stopping\n",
      "61/61 [==============================] - 1s 16ms/step - loss: 1.9282 - acc: 0.5008\n",
      "1.928236961364746 0.5007768273353577\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/henrystoll/nlp-embeddings-mlp/eb7d4fea4aeb4cdaaa402ce9cffa7dce\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     test_accuracy             : 0.5007768273353577\n",
      "COMET INFO:     test_loss                 : 1.928236961364746\n",
      "COMET INFO:     train_acc [34]            : (0.3959592580795288, 0.9941288232803345)\n",
      "COMET INFO:     train_batch_acc [170]     : (0.140625, 1.0)\n",
      "COMET INFO:     train_batch_loss [170]    : (0.21079657971858978, 10.030303955078125)\n",
      "COMET INFO:     train_epoch_duration [34] : (7.0390585759998885, 8.972409934000098)\n",
      "COMET INFO:     train_loss [34]           : (0.2169521301984787, 6.832681655883789)\n",
      "COMET INFO:     train_val_acc [34]        : (0.33661314845085144, 0.5147591829299927)\n",
      "COMET INFO:     train_val_loss [34]       : (1.4998977184295654, 7.02463436126709)\n",
      "COMET INFO:     validate_batch_acc [68]   : (0.328125, 0.6171875)\n",
      "COMET INFO:     validate_batch_loss [68]  : (1.3309941291809082, 7.0798845291137695)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     train_trainable_params : 9002104\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Nadam_beta_1           : 0.9\n",
      "COMET INFO:     Nadam_beta_2           : 0.999\n",
      "COMET INFO:     Nadam_decay            : 0.004\n",
      "COMET INFO:     Nadam_epsilon          : 1e-07\n",
      "COMET INFO:     Nadam_learning_rate    : 0.001\n",
      "COMET INFO:     Optimizer              : Nadam\n",
      "COMET INFO:     activation             : selu\n",
      "COMET INFO:     batch_size             : 128\n",
      "COMET INFO:     class_weight           : 1\n",
      "COMET INFO:     dropout                : 0.2\n",
      "COMET INFO:     embedding_dim          : 50\n",
      "COMET INFO:     epochs                 : 200\n",
      "COMET INFO:     kernel_initializer     : lecun_normal\n",
      "COMET INFO:     l2_lambda              : 0.001\n",
      "COMET INFO:     max_tokens             : 20000\n",
      "COMET INFO:     n_layers               : 1\n",
      "COMET INFO:     optimizer              : nadam\n",
      "COMET INFO:     output_sequence_length : 400\n",
      "COMET INFO:     train_Nadam_name       : Nadam\n",
      "COMET INFO:     train_steps            : 46\n",
      "COMET INFO:     units                  : 400\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     asset                    : 2\n",
      "COMET INFO:     confusion-matrix         : 1\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (241 KB)\n",
      "COMET INFO:     histogram3d              : 210\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: The Python SDK has 10800 seconds to finish before aborting...\n",
      "COMET INFO: Still uploading 1 file(s)\n"
     ]
    }
   ],
   "source": [
    "project_name = 'nlp_embeddings_mlp'\n",
    "experiment = Experiment(\n",
    "    project_name=project_name,\n",
    "    auto_param_logging=True,\n",
    "    # auto_histogram_weight_logging=True,\n",
    "    auto_histogram_gradient_logging=True,\n",
    "    auto_histogram_activation_logging=True,\n",
    "    api_key=\"HeH9EtfDC2KUlCOjeQaU1CuOM\",\n",
    "    workspace=\"henrystoll\",\n",
    ")\n",
    "params = {\n",
    "    'n_layers': n_layers,\n",
    "    'batch_size': batch_size,\n",
    "    'max_tokens': max_tokens,\n",
    "    'output_sequence_length': output_sequence_length,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'units': units,\n",
    "    'activation': activation,\n",
    "    'kernel_initializer': kernel_initializer,\n",
    "    'l2_lambda': l2_lambda,\n",
    "    'dropout': dropout,\n",
    "    'class_weight': class_weight,\n",
    "    'optimizer': optimizer,\n",
    "    'epochs': epochs,\n",
    "}\n",
    "\n",
    "experiment.log_parameters(params)\n",
    "\n",
    "model = make_model()\n",
    "keras.utils.plot_model(model, \"model-mlp.png\", show_shapes=True)\n",
    "experiment.log_asset(\"model-mlp.png\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "with experiment.train():\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        # class_weight=class_weight,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "with experiment.test():\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(loss, accuracy)\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    experiment.log_metrics(metrics)\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted = y_predicted.argmax(axis=1)\n",
    "experiment.log_confusion_matrix(y_test.to_numpy(), y_predicted)\n",
    "\n",
    "experiment.end()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3810jvsc74a57bd068e814f49104187127eba2b336217c32b637ef1ee9748ac640db13a1200321a3",
   "display_name": "Python 3.8.10 64-bit ('tensorflow': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}